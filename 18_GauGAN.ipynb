{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GauGAN\n",
    "\n",
    "It is recommended that you should already be familiar with:\n",
    " - Pix2PixHD, from [High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs](https://arxiv.org/abs/1711.11585) (Wang et al. 2018)\n",
    " - Synchronized batch norm. See Pytorch's [SyncBatchNorm](https://pytorch.org/docs/stable/generated/torch.nn.SyncBatchNorm.html) documentation.\n",
    " - Kullbach-Leibler divergence\n",
    "\n",
    "### Goals\n",
    "\n",
    "In this notebook, you will learn about GauGAN, which synthesizes high-resolution images from semantic label maps, which you implement and train. GauGAN is based around a special denormalization technique proposed in [Semantic Image Synthesis with Spatially-Adaptive Normalization](https://arxiv.org/abs/1903.07291) (Park et al. 2019)\n",
    "\n",
    "### Background\n",
    "GauGAN builds on Pix2PixHD but simplifies the overall network by adding spatially adaptive denormalization layers. Because it learns its denormalization parameters via convolving the instance segmentation map, it actually is better for multi-modal synthesis, since all it needs as is a random noise vector. Later in the notebook, you will see how the authors further control diversity with the noise vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synchronized BatchNorm\n",
    "\n",
    "So you've already heard of batch norm, which is a normalization technique that tries to normalize the statistics of activations a standard Gaussian distribution.\n",
    "\n",
    "Batch norm, however, performs poorly with small batch sizes. This becomes a problem when training large models that can only fit small batch sizes on GPUs. Training on multiple GPUs will increase the effective batch size, but vanilla batch norm will only update its statistics asynchronously on each GPU. Essentially, if you train on 2 gpus with `nn.BatchNorm2d`, the two batchnorm modules will have a different running averages of statistics and batch norm stability isn't better from larger effective batch size.\n",
    "\n",
    "Synchronized batch norm ([nn.SyncBatchNorm](https://pytorch.org/docs/stable/generated/torch.nn.SyncBatchNorm.html)) does exactly what its name suggests - it synchronizes batch norm running average updates across multiple processes so that each update will be with the statistics across all your minibatches.\n",
    "\n",
    "The authors report slightly better scores with synchronized batch norm as opposed to regular (asynchronous) batch norm. Since you will likely be running this on one machine, this notebook will stick to regular `nn.BatchNorm2d` modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatially Adaptive Denormalization (SPADE)\n",
    "\n",
    "Recall that normalization layers are formulated as\n",
    "\n",
    "\\begin{align*}\n",
    "    y &= \\dfrac{x - \\hat{\\mu}}{\\hat{\\sigma}} * \\gamma + \\beta\n",
    "\\end{align*}\n",
    "\n",
    "where $\\hat{\\mu}$ and $\\hat{\\sigma}$ correspond to an exponential moving average of minibatch means and standard deviations and are used to normalize the input activation $x$. The parameters $\\gamma$ and $\\beta$ apply \"denormalization,\" essentially allowing the model to invert the normalization if necessary.\n",
    "\n",
    "In GauGAN, batch norm is the preferred normalization scheme. Recall that batch norm can be formulated for each input neuron as\n",
    "\n",
    "\\begin{align*}\n",
    "    y_{c,h,w} &= \\dfrac{x_{c,h,w} - \\hat{\\mu}_c}{\\hat{\\sigma}_c} * \\gamma_c + \\beta_c\n",
    "\\end{align*}\n",
    "\n",
    "where $\\hat{\\mu}_c$ and $\\hat{\\sigma}_c$ are per-channel statistics computed across the batch and spatial dimensions. Similarly, $\\gamma_c$ and $\\beta_c$ are per-channel denormalization parameters.\n",
    "\n",
    "With vanilla batch norm, these denormalization parameters are spatially invariant - that is, the same values are applied to every position in the input activation. As you may imagine, this could be limiting for the model. Oftentimes it's conducive for the model to learn denormalization parameters for each position.\n",
    "\n",
    "The authors address this with **SPatially Adaptive DEnormalization (SPADE)**. They compute denormalization parameters $\\gamma$ and $\\beta$ by convolving the input segmentation masks and apply these elementwise. SPADE can therefore be formulated as\n",
    "\n",
    "\\begin{align*}\n",
    "    y_{c,h,w} &= \\dfrac{x_{c,h,w} - \\hat{\\mu}_c}{\\hat{\\sigma}_c} * \\gamma_{c,h,w} + \\beta_{c,h,w}\n",
    "\\end{align*}\n",
    "\n",
    "Now let's implement SPADE!\n",
    "\n",
    "Note: the authors use spectral norm in all convolutional layers in the generator and discriminator, but the official code omits spectral norm for SPADE layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fastai]",
   "language": "python",
   "name": "conda-env-fastai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
